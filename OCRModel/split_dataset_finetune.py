import os
import shutil
import pandas as pd
from sklearn.model_selection import StratifiedShuffleSplit
from tqdm import tqdm
from collections import Counter

# ===================================================================
# --- 1. é…ç½® (æ‚¨å”¯ä¸€éœ€è¦ä¿®æ”¹çš„åœ°æ–¹) ---
# ===================================================================

# è¾“å…¥ï¼šæ‚¨ä¹‹å‰ç”Ÿæˆçš„ã€åŒ…å«æ‰€æœ‰å­—ä½“å›¾ç‰‡çš„æ•°æ®é›†è·¯å¾„
INPUT_DIR = "../stratified_dataset" 

# è¾“å‡ºï¼šå°†è¦ç”Ÿæˆçš„ã€åˆ’åˆ†å¥½çš„æ–°æ•°æ®é›†çš„æ–‡ä»¶å¤¹åç§°
OUTPUT_DIR = "../stratified_dataset_split"

# åˆ’åˆ†æ¯”ä¾‹ï¼š0.2ä»£è¡¨ 20% çš„æ•°æ®ä½œä¸ºéªŒè¯é›†ï¼Œ80%ä½œä¸ºè®­ç»ƒé›†
VALIDATION_SET_SIZE = 0.2

# ===================================================================

# è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå¤„ç†æ¯ä¸ªåˆ’åˆ†
def process_split(dataframe, split_name):
    """
    ä¸ºCRNNæ¨¡å‹åˆ›å»ºä¸€ä¸ªåˆ’åˆ†å¥½çš„æ•°æ®é›†å­ç›®å½• (train æˆ– val)ã€‚
    å®ƒä¼šåˆ›å»ºä¸€ä¸ª images/ æ–‡ä»¶å¤¹å’Œ ä¸€ä¸ªæ€»çš„ labels.txt æ–‡ä»¶ã€‚
    """
    split_dir = os.path.join(OUTPUT_DIR, split_name)
    split_image_dir = os.path.join(split_dir, "images")
    os.makedirs(split_image_dir, exist_ok=True)
    
    output_labels_path = os.path.join(split_dir, 'labels.txt')
    dataframe[['filepath', 'transcription']].to_csv(
        output_labels_path, sep='\t', header=False, index=False
    )

    for _, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f"å¤åˆ¶ {split_name} å›¾ç‰‡"):
        source_path = os.path.join(INPUT_DIR, row['filepath'])
        dest_path = os.path.join(split_image_dir, os.path.basename(row['filepath']))
        if os.path.exists(source_path):
            shutil.copyfile(source_path, dest_path)
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°æºæ–‡ä»¶ {source_path}")

def create_split_dataset():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ‰€æœ‰æ“ä½œã€‚
    """
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®é›†åˆ†å±‚åˆ’åˆ†...")

    # åŠ è½½å¹¶è§£æåŸå§‹æ ‡ç­¾æ–‡ä»¶ ---
    labels_path = os.path.join(INPUT_DIR, "labels.txt")
    if not os.path.exists(labels_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ ‡ç­¾æ–‡ä»¶ '{labels_path}'ã€‚")
        return

    df = pd.read_csv(labels_path, sep='\t', header=None, names=['filepath', 'transcription'])
    df['font_type'] = df['filepath'].apply(lambda x: os.path.basename(x).split('_')[0])
    
    print("\nğŸ“Š åŸå§‹æ•°æ®é›†å­—ä½“åˆ†å¸ƒ:")
    print(df['font_type'].value_counts())

    # æ‰§è¡Œåˆ†å±‚é‡‡æ · ---
    print(f"\nâš™ï¸ æ­£åœ¨æŒ‰ {1-VALIDATION_SET_SIZE:.0%}/{VALIDATION_SET_SIZE:.0%} çš„æ¯”ä¾‹è¿›è¡Œåˆ†å±‚...")
    splitter = StratifiedShuffleSplit(n_splits=1, test_size=VALIDATION_SET_SIZE, random_state=42)
    train_indices, val_indices = next(splitter.split(df, df['font_type']))
    train_df = df.iloc[train_indices]
    val_df = df.iloc[val_indices]
    print(f"åˆ’åˆ†å®Œæˆ: {len(train_df)} è®­ç»ƒæ ·æœ¬, {len(val_df)} éªŒè¯æ ·æœ¬ã€‚")
    print("\nğŸ“Š éªŒè¯é›†å­—ä½“åˆ†å¸ƒ (æ£€æŸ¥å‡è¡¡æ€§):")
    print(val_df['font_type'].value_counts())
    
    # --- åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„å¹¶å¤åˆ¶æ–‡ä»¶ ---
    print("\nğŸ“‚ æ­£åœ¨åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„å¹¶å¤åˆ¶æ–‡ä»¶...")
    process_split(train_df, "train")
    process_split(val_df, "val")
    
    print(f"\nğŸ‰ğŸ‰ğŸ‰ æˆåŠŸï¼æ¨¡å‹çš„è®­ç»ƒ/éªŒè¯æ•°æ®é›†å·²åœ¨ '{OUTPUT_DIR}' æ–‡ä»¶å¤¹ä¸­åˆ›å»ºã€‚")

if __name__ == "__main__":
    create_split_dataset()