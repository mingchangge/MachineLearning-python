# è®­ç»ƒæ–‡æœ¬è¯†åˆ«æ¨¡å‹

## é˜¶æ®µä¸€ï¼šåˆæˆé«˜è´¨é‡çš„OCRè®­ç»ƒæ•°æ®

è¿™æ˜¯æ•´ä¸ªæµç¨‹ä¸­æœ€å…·åˆ›é€ æ€§ä¹Ÿæœ€å…³é”®çš„ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç¼–å†™ä¸€ä¸ªPythonè„šæœ¬ï¼Œç”Ÿæˆæˆåƒä¸Šä¸‡å¼ ä¸æ‚¨Appæˆªå›¾é£æ ¼ç±»ä¼¼çš„ã€å¸¦æœ‰æ ‡æ³¨çš„æ–‡æœ¬å°å›¾ç‰‡ã€‚

### æ­¥éª¤ 1: å‡†å¤‡ç¯å¢ƒå’Œç´ æ

1.  **å®‰è£…åº“**: æ‚¨éœ€è¦ `Pillow` åº“æ¥åˆ›å»ºå’Œç»˜åˆ¶å›¾åƒã€‚
    ```bash
    pip install Pillow
    ```

2.  **è·å–å­—ä½“**: è¿™æ˜¯å†³å®šåˆæˆæ•°æ®è´¨é‡çš„çµé­‚ã€‚
    *   **æœ€ä½³é€‰æ‹©**: å¦‚æœèƒ½é€šè¿‡è§£åŒ…Appæˆ–åœ¨ç½‘ä¸Šæ‰¾åˆ°Appä½¿ç”¨çš„å­—ä½“ï¼ˆä¾‹å¦‚ `PingFang SC`, `Helvetica Neue` ç­‰ï¼‰ï¼Œæ•ˆæœæœ€å¥½ã€‚
    *   **å¤‡é€‰æ–¹æ¡ˆ**: ä½¿ç”¨é«˜è´¨é‡çš„å¼€æºä¸­æ–‡å­—ä½“ã€‚**æ€æºé»‘ä½“ (Source Han Sans / Noto Sans CJK)** æ˜¯ä¸€ä¸ªç»ä½³çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå­—å½¢æ ‡å‡†ä¸”è¦†ç›–å­—ç¬¦å¹¿ã€‚è¯·ä¸‹è½½å¹¶å°†å…¶ `.otf` æˆ– `.ttf` æ–‡ä»¶æ”¾åœ¨æ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ä¸­ã€‚

3.  **å®šä¹‰å­—ç¬¦é›†å’Œæ ·å¼**:
    *   **å­—ç¬¦é›† (`CHARSET`)**: ä»”ç»†æŸ¥çœ‹æ‚¨çš„æˆªå›¾ï¼Œåˆ—å‡ºæ‰€æœ‰å‡ºç°è¿‡çš„å­—ç¬¦ã€‚ä¸€ä¸ªéƒ½ä¸èƒ½æ¼ï¼
        ```
        0123456789.% å…¬æ–¤èƒ–ä½æ ‡å»ºè®®å‡†é«˜ä½“è„‚ç‡æ°´åˆ†éª¨éª¼è‚Œè›‹ç™½è‚‰å†…è„æŒ‡æ•°çš®ä¸‹å»èº«å¹´é¾„å‹åŸºç¡€ä»£è°¢æ´»åŠ¨é‡æ§åˆ¶å¡
        ```
    *   **èƒŒæ™¯é¢œè‰² (`BG_COLORS`)**: ä½¿ç”¨é¢œè‰²æ‹¾å–å·¥å…·ä»æˆªå›¾ä¸­æå–ä¸»è¦èƒŒæ™¯è‰²ã€‚
        *   æ ‡å‡†ç»¿: `(47, 182, 128)`
        *   åä½è“: `(64, 169, 237)`
        *   åé«˜/èƒ–æ©™: `(245, 166, 35)`
        *   ç™½è‰²/ç°è‰²: `(255, 255, 255)`, `(240, 240, 240)`
    *   **æ–‡æœ¬é¢œè‰² (`TEXT_COLORS`)**: ä¸»è¦æ˜¯ç™½è‰² `(255, 255, 255)` å’Œæ·±ç°è‰² `(80, 80, 80)`ã€‚

### æ­¥éª¤ 2: ç¼–å†™æ•°æ®ç”Ÿæˆè„šæœ¬

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´ã€æ³¨é‡Šè¯¦ç»†çš„Pythonè„šæœ¬ã€‚å°†å®ƒä¿å­˜ä¸º `generate_ocr_data.py`ã€‚

```python
import os
import random
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import albumentations as A
import colorsys

# ==============================
# 1. é…ç½®
# ==============================
OUTPUT_DIR = 'synthetic_ocr_dataset_advanced'
NUM_IMAGES_TO_GENERATE = 10000
IMAGE_WIDTH = 200
IMAGE_HEIGHT = 50

# å­—ä½“è·¯å¾„ï¼ˆè¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨ï¼‰
FONT_PATHS = [
    './fonts/vivoSansGlobal-Regular.ttf',
    './fonts/vivoSansComp400_0.ttf',
    './fonts/vivoSansComp800_0.ttf'
]  # æ¨èä½¿ç”¨æ€æºé»‘ä½“æˆ–é˜¿é‡Œå·´å·´æ™®æƒ ä½“

# å­—ç¬¦é›†ï¼ˆæ¥è‡ªå¥åº·ç±»Appï¼‰
CHARSET = "0123456789.%BMIå¯¹æ¯”ä¸Šæ¬¡æµ‹é‡ä½“é‡å…¬æ–¤è„‚è‚ªç‡æ°´åˆ†éª¨éª¼è‚Œè›‹ç™½è´¨è‚‰å†…è„æŒ‡æ•°çš®ä¸‹å»èº«å¹´é¾„å‹åŸºç¡€ä»£è°¢æ´»åŠ¨å»ºè®®æ§åˆ¶åèƒ–é«˜ä½æ ‡å‡†è‚¥å¤§å¡éšå½¢å¾®ç¨ç˜¦å¼ºå£®è¿‡åŠ›å‘è¾¾"
STATUS_WORDS = {'åèƒ–', 'åç˜¦', 'æ ‡å‡†', 'åé«˜', 'åä½', 'æ­£å¸¸', 'å‘è¾¾', 'å¼ºå£®', 'éšå½¢', 'å¾®', 'ç¨'}
# ä»çœŸå®Appæˆªå›¾ä¸­æå–çš„èƒŒæ™¯è‰²ï¼ˆç»¿è‰²/è“è‰²/æ©™è‰²/ç™½è‰²ç³»ï¼‰
BG_COLORS = [
    (47, 182, 128), (45, 175, 122), (50, 188, 135), (42, 155, 75), (42, 154, 74),  # ç»¿è‰²ç³»
    (64, 169, 237), (60, 162, 228), (70, 175, 242), (73, 184, 255),
    (43, 96, 128), (50, 107, 140), (47, 99, 131), (35, 85, 115),                # è“è‰²ç³»
    (239, 133, 25), (245, 166, 35), (238, 160, 30), (250, 172, 45),            # æ©™è‰²ç³»
    (250, 250, 250), (245, 245, 245)                                            # ç™½è‰²ç³»
]

TEXT_COLORS = {
    'dark': (80, 80, 80),
    'light': (255, 255, 255),
    'blue': (68, 108, 141)
}

# ==============================
# 2. å·¥å…·å‡½æ•°
# ==============================

def get_font_for_text(text):
    """æ ¹æ®æ–‡æœ¬å†…å®¹é€‰æ‹©åˆé€‚çš„å­—ä½“"""
    if any(c.isdigit() for c in text) or any(word in text for word in STATUS_WORDS):
        # æ•°å€¼æˆ–çŠ¶æ€è¯ â†’ ä½¿ç”¨åŠ ç²—å­—ä½“
        return random.choice([FONT_PATHS[1], FONT_PATHS[2]])  # comp400 æˆ– comp800
    else:
        # å­—æ®µå â†’ ä½¿ç”¨å¸¸è§„å­—ä½“
        return FONT_PATHS[0]  # global-regular

def is_value_text(text):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼æˆ–çŠ¶æ€è¯"""
    if any(c.isdigit() for c in text) or any(word in text for word in STATUS_WORDS):
        return True
    return False

def choose_text_color(text, bg_color):
    """æ ¹æ®èƒŒæ™¯é¢œè‰²å’Œæ–‡æœ¬å†…å®¹é€‰æ‹©æ–‡å­—é¢œè‰²"""
    # ç‰¹å®šèƒŒæ™¯é¢œè‰²é›†åˆ
    special_bg_colors = {
        (255, 165, 0),  # æ©™è‰²
        (0, 0, 255),    # è“è‰²
        (0, 128, 0)     # ç»¿è‰²
    }
    
    if is_dark_background(bg_color):
        return TEXT_COLORS['light']
    else:
        if is_value_text(text):
            if tuple(bg_color) in special_bg_colors:
                return TEXT_COLORS['light']  # å¦‚æœèƒŒæ™¯æ˜¯ç‰¹æ®Šé¢œè‰²ä¹‹ä¸€ï¼Œä¸”æ˜¯å€¼æˆ–çŠ¶æ€è¯ï¼Œä½¿ç”¨ç™½è‰²
            else:
                return TEXT_COLORS['blue']  # å¦åˆ™ä½¿ç”¨è“è‰²
        else:
            return TEXT_COLORS['dark']  # å­—æ®µåä½¿ç”¨é»‘è‰²
        
def perturb_color_safely(rgb, sat_shift=0.06, val_shift=0.08):
    """åœ¨ä¿æŒè‰²ç›¸ä¸å˜çš„å‰æä¸‹ï¼Œå¯¹é¥±å’Œåº¦å’Œæ˜åº¦åšè½»å¾®æ‰°åŠ¨"""
    r, g, b = [x / 255.0 for x in rgb]
    h, s, v = colorsys.rgb_to_hsv(r, g, b)
    s = np.clip(s + random.uniform(-sat_shift, sat_shift), 0.0, 1.0)
    v = np.clip(v + random.uniform(-val_shift, val_shift), 0.0, 1.0)
    r, g, b = colorsys.hsv_to_rgb(h, s, v)
    return (int(round(r * 255)), int(round(g * 255)), int(round(b * 255)))

def is_dark_background(bg_color, threshold=130):
    """ä½¿ç”¨æ„ŸçŸ¥äº®åº¦å…¬å¼åˆ¤æ–­èƒŒæ™¯æ˜¯å¦ä¸ºæš—è‰²"""
    luminance = 0.299 * bg_color[0] + 0.587 * bg_color[1] + 0.114 * bg_color[2]
    return luminance < threshold

# ==============================
# 3. Albumentations å¢å¼ºç®¡é“ï¼ˆOCRå‹å¥½ï¼‰
# ==============================
transform = A.Compose([
    A.GaussianBlur(blur_limit=(3, 5), p=0.6),
    A.MotionBlur(blur_limit=(3, 5), p=0.3),
    A.ImageCompression(quality_lower=82, quality_upper=98, p=0.7),  # ä¸­æ–‡éœ€è¾ƒé«˜ç”»è´¨
    A.GaussNoise(var_limit=(5.0, 30.0), p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),
    # æ³¨æ„ï¼šç§»é™¤äº† GridDistortion å’Œ OpticalDistortion â€”â€” å¯¹ä¸­æ–‡æ˜“é€ æˆç¬”ç”»æ–­è£‚
    # A.GridDistortion(num_steps=5, distort_limit=0.1, p=0.3), # 30%çš„æ¦‚ç‡åº”ç”¨è½»å¾®çš„ç½‘æ ¼å¤±çœŸï¼Œæ¨¡æ‹Ÿå±å¹•å˜å½¢
    # A.OpticalDistortion(distort_limit=0.1, shift_limit=0.1, p=0.3), # 30%çš„æ¦‚ç‡åº”ç”¨å…‰å­¦ç•¸å˜ï¼Œæ¨¡æ‹Ÿé•œç‰‡æ•ˆæœ
])

# ==============================
# 4. ä¸»ç”Ÿæˆå‡½æ•°
# ==============================
def generate_synthetic_data_advanced():
    # åˆ›å»ºè¾“å‡ºç›®å½•
    images_dir = os.path.join(OUTPUT_DIR, 'images')
    os.makedirs(images_dir, exist_ok=True)
    labels_file_path = os.path.join(OUTPUT_DIR, 'labels.txt')

    # æ£€æŸ¥å­—ä½“æ˜¯å¦å­˜åœ¨
    for font_path in FONT_PATHS:
        if not os.path.exists(font_path):
            raise FileNotFoundError(f"å­—ä½“æ–‡ä»¶æœªæ‰¾åˆ°: {font_path}ã€‚è¯·ä¸‹è½½å¹¶æ”¾åœ¨å¯¹åº”è·¯å¾„ã€‚")

    with open(labels_file_path, 'w', encoding='utf-8') as labels_file:
        for i in range(NUM_IMAGES_TO_GENERATE):
            # 1. ç”Ÿæˆéšæœºæ–‡æœ¬
            text_length = random.randint(1, 8)
            text = ''.join(random.choices(CHARSET, k=text_length))

            # 2. é€‰æ‹©å¹¶æ‰°åŠ¨èƒŒæ™¯è‰²
            base_bg = random.choice(BG_COLORS)
            bg_color = perturb_color_safely(base_bg)

            # 3. æ ¹æ®èƒŒæ™¯äº®åº¦é€‰æ‹©æ–‡å­—é¢œè‰²
            text_color = choose_text_color(text, bg_color)

            # 4. åˆ›å»ºå›¾åƒå¹¶ç»˜åˆ¶æ–‡å­—
            image = Image.new('RGB', (IMAGE_WIDTH, IMAGE_HEIGHT), color=bg_color)
            draw = ImageDraw.Draw(image)
            font_size = random.randint(28, 36)
            font = ImageFont.truetype(font_path, font_size)

            # è®¡ç®—æ–‡æœ¬å°ºå¯¸ï¼ˆå…¼å®¹æ–°æ—§Pillowç‰ˆæœ¬ï¼‰
            try:
                bbox = draw.textbbox((0, 0), text, font=font)
                text_width = bbox[2] - bbox[0]
                text_height = bbox[3] - bbox[1]
            except AttributeError:
                text_width, text_height = draw.textsize(text, font=font)

            position = ((IMAGE_WIDTH - text_width) // 2, (IMAGE_HEIGHT - text_height) // 2)
            draw.text(position, text, font=font, fill=text_color)

            # 5. åº”ç”¨ Albumentations å¢å¼º
            image_np = np.array(image)
            transformed = transform(image=image_np)
            augmented_image_np = transformed['image']
            final_image = Image.fromarray(augmented_image_np)

            # 6. ä¿å­˜
            image_name = f'synth_{i:06d}.png'
            image_path = os.path.join(images_dir, image_name)
            final_image.save(image_path, optimize=True)

            relative_path = os.path.join('images', image_name)
            labels_file.write(f'{relative_path}\t{text}\n')

            if (i + 1) % 500 == 0:
                print(f'âœ… å·²ç”Ÿæˆ {i + 1}/{NUM_IMAGES_TO_GENERATE} å¼ å›¾ç‰‡...')

    print(f'ğŸ‰ åˆæˆæ•°æ®é›†ç”Ÿæˆå®Œæˆï¼è·¯å¾„: {os.path.abspath(OUTPUT_DIR)}')

# ==============================
# 5. å…¥å£
# ==============================
if __name__ == '__main__':
    generate_synthetic_data_advanced()

```

**å¦‚ä½•ä½¿ç”¨**:
1.  å°†ä»£ç ä¿å­˜ä¸º `generate_ocr_data.py`ã€‚
2.  ä¸‹è½½vivo Sansï¼ˆæˆ–æ‚¨é€‰æ‹©çš„å­—ä½“ï¼‰ï¼Œå°†å…¶ `.ttf` æ–‡ä»¶ä¸è„šæœ¬æ”¾åœ¨åŒä¸€ç›®å½•ã€‚
3.  è¿è¡Œè„šæœ¬: `python generate_ocr_data.py`ã€‚
4.  è¿è¡Œç»“æŸåï¼Œæ‚¨ä¼šå¾—åˆ°ä¸€ä¸ª `synthetic_ocr_dataset` æ–‡ä»¶å¤¹ï¼Œé‡Œé¢æœ‰ä¸€ä¸ª `images` å­æ–‡ä»¶å¤¹å’Œ `labels.txt` æ–‡ä»¶ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å®Œç¾çš„è®­ç»ƒæ•°æ®ï¼

---

## é˜¶æ®µäºŒï¼šé€‰æ‹©æ¨¡å‹æ¶æ„ä¸è®­ç»ƒ

### æ¨¡å‹æ¨è: è½»é‡çº§ CRNN + CTC Loss

è¿™æ˜¯æ–‡æœ¬è¯†åˆ«é¢†åŸŸçš„é»„é‡‘æ ‡å‡†ç»„åˆã€‚

*   **CRNN (Convolutional Recurrent Neural Network)**:
    1.  **C (Convolutional / å·ç§¯) éƒ¨åˆ†**: æ¨¡å‹çš„â€œçœ¼ç›â€ã€‚å®ƒç”±ä¸€ç³»åˆ—å·ç§¯å±‚ç»„æˆï¼ˆä¾‹å¦‚ä¸€ä¸ªè½»é‡çº§çš„MobileNetæˆ–ResNetä¸»å¹²ï¼‰ï¼Œè´Ÿè´£ä»è¾“å…¥å›¾åƒä¸­æå–ä¸€ç³»åˆ—ç‰¹å¾å›¾ã€‚å®ƒä¸å…³å¿ƒæ˜¯ä»€ä¹ˆå­—ï¼Œåªè´Ÿè´£æå–ç¬”ç”»ã€è¾¹ç¼˜ç­‰è§†è§‰æ¨¡å¼ã€‚
    2.  **R (Recurrent / å¾ªç¯) éƒ¨åˆ†**: æ¨¡å‹çš„â€œåºåˆ—å¤„ç†å™¨â€ã€‚é€šå¸¸ä½¿ç”¨ **åŒå‘LSTM (Bi-LSTM)**ã€‚å®ƒæ¥æ”¶æ¥è‡ªCNNçš„ç‰¹å¾åºåˆ—ï¼Œå¹¶å­¦ä¹ å­—ç¬¦ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œå…¬â€åé¢å¾ˆå¯èƒ½è·Ÿâ€œæ–¤â€ï¼‰ã€‚
    3.  **è½¬å½•å±‚**: ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥å±‚ï¼Œå°†RNNçš„è¾“å‡ºè½¬æ¢ä¸ºæ¯ä¸ªæ—¶é—´æ­¥ä¸Šå¯¹åº”æ‰€æœ‰å­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€‚

*   **CTC Loss (Connectionist Temporal Classification Loss)**:
    *   **æ ¸å¿ƒä½œç”¨**: è¿™æ˜¯CRNNçš„â€œé­”æ³•â€æ‰€åœ¨ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨**ä¸çŸ¥é“æ¯ä¸ªå­—ç¬¦åœ¨å›¾ç‰‡ä¸­ç¡®åˆ‡ä½ç½®**çš„æƒ…å†µä¸‹è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚æ‚¨åªéœ€è¦æä¾›ä¸€å¼ å›¾ç‰‡å’Œå®ƒå¯¹åº”çš„å®Œæ•´æ–‡æœ¬å­—ç¬¦ä¸²ï¼ŒCTC Lossä¼šè‡ªåŠ¨å¤„ç†å¯¹é½é—®é¢˜ã€‚è¿™æå¤§åœ°ç®€åŒ–äº†æ•°æ®æ ‡æ³¨å’Œè®­ç»ƒè¿‡ç¨‹ã€‚

### å…·ä½“è®­ç»ƒæ“ä½œ (ä»¥Kaggle Notebookä¸ºä¾‹)

è®­ç»ƒCRNNæ¯”è°ƒç”¨YOLOæ¡†æ¶è¦å¤æ‚ä¸€äº›ï¼Œå› ä¸ºå®ƒéœ€è¦æˆ‘ä»¬è‡ªå·±å®šä¹‰æ¨¡å‹å’Œè®­ç»ƒå¾ªç¯ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ **PyTorch**ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸çµæ´»ä¸”å¼ºå¤§çš„æ¡†æ¶ã€‚

1.  **å‡†å¤‡æ•°æ®**:
    *   å°†æ‚¨ç”Ÿæˆçš„ `synthetic_ocr_dataset` æ–‡ä»¶å¤¹å‹ç¼©æˆ `.zip`ã€‚åœ¨macç³»ç»Ÿå¯ä»¥ä½¿ç”¨ `zip -r synthetic_ocr_dataset_advanced.zip synthetic_ocr_dataset_advanced` å‘½ä»¤ã€‚
    *   åœ¨Kaggleä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„**ç§æœ‰æ•°æ®é›†**å¹¶ä¸Šä¼ è¿™ä¸ªzipæ–‡ä»¶ã€‚

2.  **åˆ›å»ºå¹¶è®¾ç½®Kaggle Notebook**:
    *   åˆ›å»ºä¸€ä¸ªæ–°çš„Notebookã€‚
    *   é€šè¿‡ `+ Add Input` æ·»åŠ æ‚¨åˆšåˆšåˆ›å»ºçš„åˆæˆOCRæ•°æ®é›†ã€‚
    *   åœ¨å³ä¾§é¢æ¿è®¾ç½® `Accelerator` ä¸º **GPU**ï¼Œå¹¶ **å¼€å¯ `Internet`**ã€‚
    

3.  **ç¼–å†™è®­ç»ƒä»£ç  (åœ¨Notebookå•å…ƒæ ¼ä¸­)**:
    è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å¯è¿è¡Œçš„PyTorchè®­ç»ƒè„šæœ¬çš„æ¡†æ¶ã€‚æ‚¨åªéœ€æŒ‰é¡ºåºå°†å…¶ç²˜è´´åˆ°Kaggle Notebookçš„å•å…ƒæ ¼ä¸­å³å¯ã€‚

    **Cell 1: å®‰è£…ä¾èµ–ä¸å¯¼å…¥åº“**
    ```python
    # å®‰è£…ä¸€ä¸ªç”¨äºè®¡ç®—æ–‡æœ¬ç¼–è¾‘è·ç¦»çš„åº“ï¼Œæ–¹ä¾¿è¯„ä¼°æ¨¡å‹
    !pip install python-Levenshtein -q

    import os
    import cv2
    import torch
    import torch.nn as nn
    from torch.utils.data import Dataset, DataLoader
    from torchvision import transforms
    import numpy as np
    from collections import OrderedDict
    import zipfile
    ```

    **Cell 2: è§£å‹æ•°æ®å¹¶è®¾ç½®é…ç½®**
    ```python
    # è§£å‹æ•°æ®é›†åˆ°å¯å†™ç›®å½•
    with zipfile.ZipFile('/kaggle/input/YOUR_SYNTHETIC_DATASET_NAME/synthetic_ocr_dataset.zip', 'r') as zip_ref:
        zip_ref.extractall('/kaggle/working/')

    # --- é…ç½® ---
    DATA_DIR = '/kaggle/working/synthetic_ocr_dataset'
    CHARSET = "0123456789.%å…¬æ–¤èƒ–ä½æ ‡å»ºè®®å‡†é«˜ä½“è„‚ç‡æ°´åˆ†éª¨éª¼è‚Œè›‹ç™½è‚‰å†…è„æŒ‡æ•°çš®ä¸‹å»èº«å¹´é¾„å‹åŸºç¡€ä»£è°¢æ´»åŠ¨é‡æ§åˆ¶å¡"
    
    # å°†å­—ç¬¦æ˜ å°„åˆ°æ•´æ•°
    # 0ä¿ç•™ç»™CTCçš„'blank' token
    char_to_int = {char: i + 1 for i, char in enumerate(CHARSET)}
    int_to_char = {i + 1: char for i, char in enumerate(CHARSET)}
    
    NUM_CLASSES = len(CHARSET) + 1 # +1 for the blank token
    IMG_WIDTH = 200
    IMG_HEIGHT = 50
    EPOCHS = 50 # å…ˆç”¨50è½®å¿«é€Ÿè¿­ä»£
    BATCH_SIZE = 64
    ```

    **Cell 3: åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ç±»**
    ```python
    class OCRDataset(Dataset):
        def __init__(self, data_dir, char_to_int_map, transform=None):
            self.data_dir = data_dir
            self.transform = transform
            self.char_to_int = char_to_int_map
            self.image_paths = []
            self.labels = []
            
            with open(os.path.join(data_dir, 'labels.txt'), 'r', encoding='utf-8') as f:
                for line in f:
                    path, label = line.strip().split('\t')
                    self.image_paths.append(os.path.join(data_dir, path))
                    self.labels.append(label)

        def __len__(self):
            return len(self.image_paths)

        def __getitem__(self, idx):
            img_path = self.image_paths[idx]
            label = self.labels[idx]
            
            # è¯»å–ç°åº¦å›¾
            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            
            if self.transform:
                image = self.transform(image)
            
            # ç¼–ç æ–‡æœ¬
            encoded_label = [self.char_to_int[char] for char in label]
            
            return {'image': image, 'label': torch.IntTensor(encoded_label), 'label_length': len(encoded_label)}

    # å®šä¹‰å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,)) # å½’ä¸€åŒ–åˆ°[-1, 1]
    ])
    ```

    **Cell 4: å®šä¹‰CRNNæ¨¡å‹æ¶æ„**
    ```python
    class CRNN(nn.Module):
        def __init__(self, num_classes):
            super(CRNN, self).__init__()
            # CNN part (simplified VGG-like)
            self.cnn = nn.Sequential(
                nn.Conv2d(1, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # 64x25x100
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # 128x12x50
                nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)), # 256x5x50
                nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)), # 512x2x50
                nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), nn.ReLU(True) # 512x1x49
            )
            # RNN part
            self.rnn = nn.Sequential(
                nn.LSTM(512, 256, bidirectional=True),
                nn.LSTM(512, 256, bidirectional=True)
            )
            # Transcription part
            self.fc = nn.Linear(512, num_classes)

        def forward(self, x):
            # CNN forward
            conv = self.cnn(x)
            b, c, h, w = conv.size()
            assert h == 1, "the height of conv feature should be 1"
            conv = conv.squeeze(2) # [b, c, w]
            conv = conv.permute(2, 0, 1) # [w, b, c] for RNN
            
            # RNN forward
            rnn, _ = self.rnn(conv)
            
            # Transcription
            output = self.fc(rnn)
            return output
    ```

    **Cell 5: è®­ç»ƒå¾ªç¯**
    ```python
    def collate_fn(batch):
        images = [item['image'] for item in batch]
        labels = [item['label'] for item in batch]
        label_lengths = [item['label_length'] for item in batch]
        
        images = torch.stack(images, 0)
        labels = torch.cat(labels, 0)
        label_lengths = torch.IntTensor(label_lengths)
        
        return images, labels, label_lengths

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = CRNN(NUM_CLASSES).to(device)
    criterion = nn.CTCLoss(blank=0, reduction='mean')
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    dataset = OCRDataset(DATA_DIR, char_to_int, transform=transform)
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† 80/20
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {EPOCHS} è½®...")
    for epoch in range(EPOCHS):
        model.train()
        for i, (images, labels, label_lengths) in enumerate(train_loader):
            images = images.to(device)
            
            preds = model(images) # [seq_len, batch_size, num_classes]
            preds_size = torch.IntTensor([preds.size(0)] * images.size(0))
            
            loss = criterion(preds.log_softmax(2), labels, preds_size, label_lengths)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if (i + 1) % 50 == 0:
                print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')
    
    print("è®­ç»ƒå®Œæˆï¼")

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), '/kaggle/working/crnn_model.pth')
    print("æ¨¡å‹å·²ä¿å­˜åˆ° /kaggle/working/crnn_model.pth")
    ```

    **å®Œæ•´ä»£ç : **

    ```python
    import os
    import cv2
    import torch
    import torch.nn as nn
    from torch.utils.data import Dataset, DataLoader, random_split
    from torchvision import transforms
    import numpy as np
    import Levenshtein
    print("--> ä¾èµ–åº“å¯¼å…¥å®Œæˆã€‚")


    # ===================================================================
    # é˜¶æ®µäºŒï¼šé…ç½®
    # ===================================================================
    print("--> æ­£åœ¨é…ç½®è·¯å¾„å’Œå‚æ•°...")
    INPUT_DATA_DIR = '/kaggle/input/ocr-dataset/synthetic_ocr_dataset_advanced'
    OUTPUT_DIR = '/kaggle/working/'

    if not os.path.exists(INPUT_DATA_DIR):
        raise FileNotFoundError(f"é”™è¯¯ï¼šæ— æ³•åœ¨ '{INPUT_DATA_DIR}' æ‰¾åˆ°æ•°æ®é›†ã€‚")
    else:
        print(f"æˆåŠŸæ‰¾åˆ°æ•°æ®é›†äº: {INPUT_DATA_DIR}")

    CHARSET = "0123456789.%BMIå¯¹æ¯”ä¸Šæ¬¡æµ‹é‡ä½“é‡å…¬æ–¤è„‚è‚ªç‡æ°´åˆ†éª¨éª¼è‚Œè›‹ç™½è´¨è‚‰å†…è„æŒ‡æ•°çš®ä¸‹å»èº«å¹´é¾„å‹åŸºç¡€ä»£è°¢æ´»åŠ¨å»ºè®®æ§åˆ¶åèƒ–é«˜ä½æ ‡å‡†è‚¥å¤§å¡éšå½¢å¾®ç¨ç˜¦å¼ºå£®è¿‡åŠ›å‘è¾¾"
    char_to_int = {char: i + 1 for i, char in enumerate(CHARSET)}
    int_to_char = {i + 1: char for i, char in enumerate(CHARSET)}
    NUM_CLASSES = len(CHARSET) + 1

    IMG_WIDTH = 200
    IMG_HEIGHT = 32
    EPOCHS = 75
    BATCH_SIZE = 128
    VAL_RATIO = 0.2

    # æ—©åœé…ç½®
    EARLY_STOPPING_PATIENCE = 10  # è¿ç»­10è½®æ— æ”¹å–„åˆ™åœæ­¢


    # ===================================================================
    # é˜¶æ®µä¸‰ï¼šå·¥å…·å‡½æ•°
    # ===================================================================
    def calculate_cer(pred_text, true_text):
        if len(true_text) == 0:
            return 0.0 if len(pred_text) == 0 else 1.0
        return Levenshtein.distance(pred_text, true_text) / len(true_text)

    def ctc_decode(predictions, int_to_char):
        preds = predictions.argmax(2).cpu().numpy()
        texts = []
        for b in range(preds.shape[1]):
            seq = preds[:, b]
            decoded = []
            prev = 0
            for idx in seq:
                if idx != 0 and idx != prev:
                    decoded.append(int_to_char.get(idx, ''))
                prev = idx
            texts.append(''.join(decoded))
        return texts


    # ===================================================================
    # é˜¶æ®µå››ï¼šæ•°æ®é›†ç±»
    # ===================================================================
    class OCRDataset(Dataset):
        def __init__(self, data_dir, char_to_int_map, transform=None):
            self.data_dir = data_dir
            self.transform = transform
            self.char_to_int = char_to_int_map
            self.image_paths = []
            self.labels = []
            labels_path = os.path.join(data_dir, 'labels.txt')
            with open(labels_path, 'r', encoding='utf-8') as f:
                for line in f:
                    parts = line.strip().split('\t')
                    if len(parts) != 2:
                        continue
                    path, label = parts
                    self.image_paths.append(os.path.join(self.data_dir, path))
                    self.labels.append(label)

        def __len__(self):
            return len(self.image_paths)

        def __getitem__(self, idx):
            img_path = self.image_paths[idx]
            label = self.labels[idx]
            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if image is None:
                image = np.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)
            if self.transform:
                image = self.transform(image)
            encoded_label = [self.char_to_int.get(char, 0) for char in label]
            return {'image': image, 'label': torch.IntTensor(encoded_label), 'label_length': len(encoded_label), 'text': label}

    def collate_fn(batch):
        # ä»batchä¸­æå–æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯
        images = [item['image'] for item in batch]
        labels = [item['label'] for item in batch]
        label_lengths = [item['label_length'] for item in batch] # <-- ä¿æŒä¸ºPythonåˆ—è¡¨
        texts = [item['text'] for item in batch]

        # å°†å›¾åƒå †å æˆä¸€ä¸ªæ‰¹æ¬¡ (è¿™éƒ¨åˆ†å¿…é¡»æ˜¯å¼ é‡)
        images = torch.stack(images, 0)
        
        # ç›´æ¥è¿”å›è¿æ¥åçš„æ ‡ç­¾å¼ é‡å’Œæ ‡ç­¾é•¿åº¦çš„Pythonåˆ—è¡¨
        labels_tensor = torch.cat(labels, 0) # æ ‡ç­¾è¿˜æ˜¯éœ€è¦è¿æ¥çš„
        
        return images, labels_tensor, label_lengths, texts


    # ===================================================================
    # é˜¶æ®µäº”ï¼šæ¨¡å‹å®šä¹‰
    # ===================================================================
    class CRNN(nn.Module):
        def __init__(self, num_classes):
            super(CRNN, self).__init__()
            # CNNéƒ¨åˆ†ä¿æŒä¸å˜ï¼Œç‰¹å¾æå–èƒ½åŠ›ä¾ç„¶å¼ºå¤§
            self.cnn = nn.Sequential(
                nn.Conv2d(1, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),
                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),
                nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),
                nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),
                nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),
                nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),
                nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)
            )
            
            # --- å…³é”®ä¿®æ”¹ ---
            # ä½¿ç”¨GRUä»£æ›¿LSTMï¼šGRUå‚æ•°æ›´å°‘ï¼Œé€šå¸¸æ€§èƒ½ç›¸å½“ï¼Œä¸”æ›´ç¨³å®š
            # å°†num_layersä»2å‡å°‘åˆ°1ï¼šå¯¹äºè¿™ä¸ªä»»åŠ¡ï¼Œå•å±‚åŒå‘GRU/LSTMé€šå¸¸è¶³å¤Ÿ
            self.rnn = nn.GRU(
                input_size=512, 
                hidden_size=256, 
                num_layers=1,         # <--- ä¿®æ”¹ç‚¹ 1
                bidirectional=True, 
                batch_first=False
            )
            
            self.fc = nn.Linear(512, num_classes) # åŒå‘ï¼Œæ‰€ä»¥ 256 * 2 = 512

        def forward(self, x):
            conv = self.cnn(x)
            b, c, h, w = conv.size()
            assert h == 1, "ç‰¹å¾å›¾é«˜åº¦å¿…é¡»ä¸º1"
            conv = conv.squeeze(2)
            conv = conv.permute(2, 0, 1) # [seq_len, batch, input_size]
            
            # GRUçš„è¾“å‡ºä¸LSTMç•¥æœ‰ä¸åŒï¼Œä½†å¯¹äºåç»­çš„å…¨è¿æ¥å±‚æ˜¯å…¼å®¹çš„
            rnn, _ = self.rnn(conv)
            
            output = self.fc(rnn)
            return output

    # ===================================================================
    # é˜¶æ®µå…­ï¼šè®­ç»ƒä¸éªŒè¯ï¼ˆå«æ—©åœ + å¯è§†åŒ–ï¼‰
    # ===================================================================
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"--> ä½¿ç”¨è®¾å¤‡: {device}")

    transform = transforms.Compose([
        transforms.ToPILImage(),  # å¿…é¡»ï¼å› ä¸º cv2 è¿”å› ndarray
        transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    full_dataset = OCRDataset(INPUT_DATA_DIR, char_to_int, transform=transform)

    val_size = int(len(full_dataset) * VAL_RATIO)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)

    model = CRNN(NUM_CLASSES).to(device)
    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)


    best_cer = float('inf')
    trigger_times = 0
    best_model_path = os.path.join(OUTPUT_DIR, 'crnn_best_model.pth')

    for epoch in range(EPOCHS):
        # ========== è®­ç»ƒé˜¶æ®µ ==========
        model.train()
        train_loss = 0.0
        for i, (images_cpu, labels_cpu, label_lengths_list, _) in enumerate(train_loader):
            # --- å…³é”®ä¿®å¤ ---
            # 1. å°†å›¾åƒç§»åŠ¨åˆ°GPU
            images = images_cpu.to(device)
            
            # 2. å°†è¿æ¥å¥½çš„æ ‡ç­¾ç§»åŠ¨åˆ°GPU
            labels = labels_cpu.to(device)                
            
            # 3. å°†æ ‡ç­¾é•¿åº¦çš„Pythonåˆ—è¡¨è½¬æ¢ä¸ºGPUä¸Šçš„LongTensor
            label_lengths = torch.tensor(label_lengths_list, dtype=torch.long).to(device)
            
            # ç°åœ¨ï¼Œæ‰€æœ‰è¾“å…¥criterionçš„å¼ é‡éƒ½åœ¨è¿›å…¥æ¨¡å‹å‰è¢«æ­£ç¡®åœ°æ”¾åœ¨äº†GPUä¸Š
            
            preds = model(images)
            T = preds.size(0)
            preds_size = torch.full((images.size(0),), T, dtype=torch.long, device=device)

            if epoch == 0 and i == 0:
                print("--- è¯Šæ–­ä¿¡æ¯ ---")
                print(f"preds device:         {preds.device}")
                print(f"labels device:        {labels.device}")
                print(f"preds_size device:    {preds_size.device}")
                print(f"label_lengths device: {label_lengths.device}")
                print("------------------")


            
            loss = criterion(preds.log_softmax(2), labels, preds_size, label_lengths)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)
            optimizer.step()
            
            train_loss += loss.item()


        # ========== éªŒè¯é˜¶æ®µ ==========
        model.eval()
        val_loss = 0.0
        total_cer = 0.0
        total_samples = 0
        all_pred_texts = []
        all_true_texts = []

        with torch.no_grad():
            for images_cpu, labels_cpu, label_lengths_list, true_texts in val_loader:
                # --- å…³é”®ä¿®å¤ (éªŒè¯å¾ªç¯) ---
                images = images_cpu.to(device)
                labels = labels_cpu.to(device)                
                label_lengths = torch.tensor(label_lengths_list, dtype=torch.long).to(device)
                
                preds = model(images)
                T = preds.size(0)
                preds_size = torch.full((images.size(0),), T, dtype=torch.long, device=device)
                
                loss = criterion(preds.log_softmax(2), labels, preds_size, label_lengths)
                val_loss += loss.item()

                pred_texts = ctc_decode(preds, int_to_char)
                all_pred_texts.extend(pred_texts)
                all_true_texts.extend(true_texts)

                for pred, true in zip(pred_texts, true_texts):
                    total_cer += calculate_cer(pred, true)
                    total_samples += 1

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        avg_cer = total_cer / total_samples

        print(f'Epoch [{epoch+1}/{EPOCHS}] '
            f'Train Loss: {avg_train_loss:.4f} | '
            f'Val Loss: {avg_val_loss:.4f} | '
            f'Val CER: {avg_cer:.4f}')

        # ========== å¯è§†åŒ–é¢„æµ‹æ ·ä¾‹ï¼ˆæ¯5ä¸ªepochï¼‰==========
        if (epoch + 1) % 5 == 0:
            print("\nğŸ” é¢„æµ‹æ ·ä¾‹ï¼ˆéªŒè¯é›†éšæœºé‡‡æ ·ï¼‰:")
            indices = np.random.choice(len(all_pred_texts), size=min(5, len(all_pred_texts)), replace=False)
            for i in indices:
                pred_txt = all_pred_texts[i]
                true_txt = all_true_texts[i]
                status = "âœ…" if pred_txt == true_txt else "âŒ"
                print(f"  {status} é¢„æµ‹: '{pred_txt}' | çœŸå®: '{true_txt}'")
            print()

        # ========== å­¦ä¹ ç‡è°ƒåº¦ + æ—©åœ ==========
        scheduler.step(avg_val_loss)

        if avg_cer < best_cer:
            best_cer = avg_cer
            trigger_times = 0
            torch.save(model.state_dict(), best_model_path)
            print(f"--> æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (CER: {best_cer:.4f})")
        else:
            trigger_times += 1
            if trigger_times >= EARLY_STOPPING_PATIENCE:
                print(f"--> â¹ï¸ æ—©åœè§¦å‘ï¼å·²è¿ç»­ {EARLY_STOPPING_PATIENCE} è½®æœªæå‡ã€‚")
                break

    print("--> è®­ç»ƒå®Œæˆï¼")
    print(f"--> æœ€ä½³éªŒè¯ CER: {best_cer:.4f}")
    print(f"--> æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path}")    
    ```

4.  **ä¿å­˜å¹¶è·å–æ¨¡å‹**:
    *   ç‚¹å‡»å³ä¸Šè§’çš„ `Save Version` -> `Save & Run All (Commit)`ã€‚
    *   ç­‰å¾…Kaggleåœ¨åå°è¿è¡Œå®Œæ¯•ã€‚
    *   å›åˆ°Notebookçš„ `Data` -> `Output` é¡µé¢ï¼Œæ‚¨å°±å¯ä»¥æ‰¾åˆ°å¹¶ä¸‹è½½è®­ç»ƒå¥½çš„ `crnn_model.pth` æ–‡ä»¶äº†ã€‚

## é˜¶æ®µä¸‰ï¼šæ¨¡å‹è½¬æ¢ (PyTorch -> ONNX -> TF.js)

ç”±äºæµè§ˆå™¨ç¯å¢ƒåŸç”Ÿæ”¯æŒTF.jsï¼Œæˆ‘ä»¬éœ€è¦å°†PyTorchæ¨¡å‹è½¬æ¢ã€‚

1.  **åœ¨Notebookä¸­å¢åŠ å¯¼å‡ºä»£ç **:
    åœ¨è®­ç»ƒä»£ç ä¸‹æ–¹æ·»åŠ æ–°çš„å•å…ƒæ ¼ï¼Œç”¨äºå°†`.pth`æ–‡ä»¶è½¬æ¢ä¸ºé€šç”¨æ ¼å¼ **ONNX**ã€‚

    ```python
    # é¦–å…ˆåŠ è½½æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹
    model.load_state_dict(torch.load('/kaggle/working/crnn_model.pth'))
    model.eval()

    # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè¾“å…¥ï¼Œå°ºå¯¸è¦å’Œæ¨¡å‹è¾“å…¥ä¸€è‡´
    dummy_input = torch.randn(1, 1, IMG_HEIGHT, IMG_WIDTH).to(device)
    
    torch.onnx.export(model,
                      dummy_input,
                      "/kaggle/working/crnn_model.onnx",
                      export_params=True,
                      opset_version=11,
                      input_names=['input'],
                      output_names=['output'],
                      dynamic_axes={'input': {0: 'batch_size'}, 'output': {1: 'batch_size'}})

    print("æ¨¡å‹å·²å¯¼å‡ºä¸º ONNX æ ¼å¼: /kaggle/working/crnn_model.onnx")
    ```

2.  **å®‰è£…ONNXåˆ°TF.jsè½¬æ¢å·¥å…·å¹¶è½¬æ¢**:

    ```bash
    !pip install onnx onnx-tf -q
    !onnx-tf convert -i /kaggle/working/crnn_model.onnx -o /kaggle/working/tfjs_model
    ```
    * è¿™æ¡å‘½ä»¤ä¼šå°† `.onnx` æ–‡ä»¶è½¬æ¢ä¸ºä¸€ä¸ªåŒ…å« `model.json` å’Œæƒé‡æ–‡ä»¶çš„ `tfjs_model` æ–‡ä»¶å¤¹ã€‚

3.  **æœ€ç»ˆä¿å­˜**:
    *   å†æ¬¡ `Save Version` -> `Save & Run All (Commit)`ã€‚
    *   å®Œæˆåï¼Œåœ¨ `Output` ä¸­ï¼Œæ‚¨å°±å¯ä»¥ä¸‹è½½æœ€ç»ˆçš„ `tfjs_model` æ–‡ä»¶å¤¹äº†ã€‚è¿™å°±æ˜¯æ‚¨çš„ç¬¬äºŒä¸ªè½»é‡åŒ–æ¨¡å‹ï¼

è¿™ä¸ªæµç¨‹è™½ç„¶æ­¥éª¤ç¹å¤šï¼Œä½†æ¯ä¸€æ­¥éƒ½æ˜¯æ„å»ºä¸€ä¸ªå¯æ§ã€é«˜æ€§èƒ½è‡ªå®šä¹‰AIæ¨¡å‹çš„æ ‡å‡†å®è·µã€‚æ‚¨ç°åœ¨æ‹¥æœ‰äº†ä»æ•°æ®åˆ›é€ åˆ°æ¨¡å‹éƒ¨ç½²çš„å…¨å¥—æŠ€èƒ½ã€‚